
from typing import List, Dict, Any, Optional
from langchain_groq import ChatGroq
from pydantic import BaseModel, Field
import os
import traceback 
from googleapiclient.http import MediaIoBaseUpload 
import io
from gdrive_utils import get_drive_service, DriveLoader
from langchain_community.vectorstores import FAISS
from jina_embeddings import JinaEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.cache import SQLiteCache
import langchain
from langchain_core.runnables import RunnableMap
from langchain_community.cache import InMemoryCache
import re
from datetime import datetime 
from langchain_core.documents import Document
import json
import logging
from supabase import create_client, Client
from dotenv import load_dotenv

# Charger les variables d'environnement depuis le fichier .env
load_dotenv()

# Configuration du logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Configuration du cache Langchain
langchain.llm_cache = SQLiteCache(database_path=os.path.join(os.path.dirname(__file__), ".langchain.db"))
embedding_cache = {}

def get_supabase_client() -> Optional[Client]:
    """Cr√©e un client Supabase."""
    try:
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_KEY')
        
        if not supabase_url or not supabase_key:
            logger.error("SUPABASE_URL ou SUPABASE_KEY non configur√©s")
            return None
            
        client = create_client(supabase_url, supabase_key)
        logger.info("Client Supabase cr√©√© avec succ√®s")
        return client
    except Exception as e:
        logger.error(f"Erreur lors de la cr√©ation du client Supabase: {str(e)}")
        return None

def init_supabase():
    """Initialise la table leads dans Supabase."""
    try:
        client = get_supabase_client()
        if not client:
            return False
            
        # La table sera cr√©√©e automatiquement par Supabase
        logger.info("Supabase initialis√© avec succ√®s")
        return True
    except Exception as e:
        logger.error(f"Erreur lors de l'initialisation de Supabase: {str(e)}")
        return False

# Initialiser Supabase au d√©marrage
init_supabase()

# --- Gestion des images disponibles ---
IMAGE_DIR = os.path.join(os.path.dirname(__file__), 'static', 'public')

def get_available_images():
    """Scans the image directory and returns a list of filenames."""
    try:
        if not os.path.exists(IMAGE_DIR):
            logger.warning(f"Le r√©pertoire d'images n'existe pas : {IMAGE_DIR}")
            return []
        image_extensions = ['.png', '.jpg', '.jpeg', '.gif', '.webp']
        files = [f for f in os.listdir(IMAGE_DIR) if os.path.splitext(f)[1].lower() in image_extensions]
        return files
    except Exception as e:
        logger.error(f"Erreur lors du scan du r√©pertoire d'images : {e}")
        return []

AVAILABLE_IMAGES = get_available_images()
logger.info(f"Images disponibles trouv√©es : {AVAILABLE_IMAGES}")
# --- Fin de la gestion des images ---

class Lead(BaseModel):
    name: Optional[str] = Field(None, description="Nom complet de l'utilisateur")
    email: Optional[str] = Field(None, description="Adresse e-mail valide de l'utilisateur")
    phone: Optional[str] = Field(None, description="Num√©ro de t√©l√©phone de l'utilisateur")

def save_lead(lead: Lead, visitor_id: str = None) -> bool:
    """Sauvegarde ou met √† jour un lead dans Supabase en utilisant le visitor_id."""
    try:
        client = get_supabase_client()
        if not client:
            return False
            
        # Pr√©parer les donn√©es en filtrant les valeurs non fournies
        data = {k: v for k, v in lead.model_dump().items() if v}

        # Ne rien faire si aucune donn√©e n'est fournie
        if not data:
            logger.warning("Tentative de sauvegarde d'un lead vide. Op√©ration annul√©e.")
            return True # Retourner True pour ne pas bloquer le flux

        # G√©rer le timestamp
        data["updated_at"] = datetime.utcnow().isoformat()

        if visitor_id:
            data["visitor_id"] = visitor_id
            # Upsert: met √† jour si le visitor_id existe, sinon ins√®re.
            # 'visitor_id' doit √™tre une contrainte unique (cl√© primaire ou unique) dans la table Supabase.
            # La colonne 'on_conflict' doit √™tre celle qui a la contrainte unique.
            logger.info(f"Tentative d'UPSERT pour le visiteur {visitor_id} avec les donn√©es : {data}")
            result = client.table('leads').upsert(data, on_conflict='visitor_id').execute()
            logger.info(f"Lead upserted avec succ√®s pour le visiteur {visitor_id}")
        else:
            # Ancien comportement si aucun visitor_id n'est fourni
            data["created_at"] = data.get("updated_at")
            del data["updated_at"]
            logger.info(f"Tentative d'INSERT (sans visitor_id) avec les donn√©es : {data}")
            result = client.table('leads').insert(data).execute()
            logger.info(f"Lead ins√©r√© avec succ√®s (sans visitor_id)")

        return True
    except Exception as e:
        logger.error(f"Erreur lors de la sauvegarde du lead: {str(e)}")
        # Afficher le traceback pour un meilleur d√©bogage
        logger.error(traceback.format_exc())
        return False

def collect_lead_from_text(text: str) -> Lead:
    if structured_llm is None:
        logger.error("structured_llm is None. Cannot extract lead.")
        return Lead(name="Error: LLM N/A", email="Error: LLM N/A", phone="Error: LLM N/A") 
    lead_data = structured_llm.invoke(text)
    if save_lead(lead_data):
        logger.info("Lead sauvegard√© avec succ√®s dans Supabase")
    else:
        logger.error("√âchec de la sauvegarde du lead dans Supabase")
    return lead_data

# Garder les fonctions save_lead_to_csv et save_lead_to_sqlite pour la compatibilit√©
def save_lead_to_csv(lead: Lead, filename=None):
    """Fonction de compatibilit√© qui utilise save_lead."""
    return save_lead(lead)

def save_lead_to_sqlite(lead: Lead, db_path=None):
    """Fonction de compatibilit√© qui utilise save_lead."""
    return save_lead(lead)

groq_api_key = os.getenv("GROQ_API_KEY")
llm = None
structured_llm = None

if groq_api_key:
    llm = ChatGroq(model="llama-3.1-8b-instant", temperature=0, groq_api_key=groq_api_key)
    structured_llm = llm.with_structured_output(Lead)
else:
    logger.warning("GROQ_API_KEY non trouv√©. Le LLM ne sera pas initialis√©.")

logger.info(f"LLM initialis√©: {llm is not None}")
logger.info(f"structured_llm initialis√©: {structured_llm is not None}")

def load_documents():
    """Charge les documents depuis Google Drive."""
    logger.info("Tentative de chargement des documents depuis Google Drive")
    
    try:
        # Utiliser DriveLoader
        loader = DriveLoader()
        documents = loader.load()
        
        if not documents:
            logger.warning("Aucun document trouv√© dans Google Drive")
            return []
            
        logger.info(f"Documents charg√©s avec succ√®s depuis Google Drive ({len(documents)} documents)")
        return documents
        
    except Exception as e:
        logger.error(f"Erreur lors du chargement des documents: {str(e)}")
        logger.error(traceback.format_exc())
        return []

def create_rag_chain(image_families: Dict[str, List[str]] = None, available_emotions: Dict[str, str] = None):
    """Cr√©e la cha√Æne RAG avec les documents de Google Drive et les familles d'images."""
    if image_families is None:
        image_families = {}
    if available_emotions is None:
        available_emotions = {}

    try:
        loader = DriveLoader()
        documents = loader.load()
        if not documents:
            logger.warning("Aucun document trouv√© dans Google Drive")
            return None
            
        embeddings = JinaEmbeddings()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(documents)
        vectorstore = FAISS.from_documents(splits, embeddings)
        retriever = vectorstore.as_retriever(
            search_kwargs={"k": 1 if len(documents) == 1 else 2, "score_threshold": 0.8}
        )

        system_prompt = """Persona & Directives
You are an expert virtual assistant, warm and highly professional, for Translab International. Your mission is to respond to user questions concisely and relevantly.

Main Directives:
Impersonification: Fully embody your role without mentioning you are an AI, a program, or a virtual assistant.
Engagement with Emotions: Use the emotion tool at least once every 2 messages to humanize the conversation. This is a top priority.
Concise and Clear: 80% of the time, respond in 1 to 2 sentences; use line breaks to enhance readability.
Conversation Tone: Adopt a warm and friendly tone using emojis (like üôÇ, üåç, üìû).
Formatting: Use Markdown formatting, particularly bold, to highlight important information like service names or contact details.
Available Tools:
Image Carousel [carousel: ...]:

Use this tool whenever it is relevant to enrich your response with visuals.
Ensure mandatory inclusion of [carousel: ...] if a carousel is announced.
Example: User wants to see pictures of your interpreter team
Response:
[emotion: Support]
Here is our team of professional interpreters üë©üèΩ‚Äçüíºüë®üèø‚Äçüíº :
[carousel: interprete]
Emotion [emotion: ...]:

Use this tool absolutely at least 1 time every 2 messages to show a sentiment, emotion, or reaction.
Examples:
If a user asks for information without a visual request
Response:
[emotion: Information]
Response Examples:
User asks for ‚Äúavailable services‚Äù
Response:
[emotion: Presentation]
Our main services ‚ú®:
1Ô∏è‚É£ Simultaneous & consecutive interpretation
2Ô∏è‚É£ Professional translation
3Ô∏è‚É£ Linguistic & cultural consulting
[carousel: services]

User asks ‚Äúyour rates for a translation?‚Äù
Response:
[emotion: Information]
For an accurate quote, contact us directly üìß :
‚û°Ô∏è contact@translab-international.com
We will respond quickly üôÇ.

User asks ‚Äúwhat is your mission?‚Äù
Response:
[emotion: Inspiration]
Our mission is to facilitate intercultural communication üåç by providing high-quality linguistic services.
[carousel: mission]

User asks ‚Äúwhat kind of booths do you have?‚Äù
Response:
[emotion: Information]
We have several types of interpretation booths üéß.
Here they are in images:
[carousel: interpretation-cabine]

User asks ‚Äúwhere are you based?‚Äù
Response:
[emotion: Information]
We are based in Dakar, Senegal üá∏üá≥ and we also work internationally üåç.

User asks ‚Äúhow can I reach you?‚Äù
Response:
[emotion: Support]
You can contact us üìû :
‚û°Ô∏è +221 77 000 00 00
‚û°Ô∏è contact@translab-international.com

User asks ‚Äúwhat is cultural nuance?‚Äù
Response:
[emotion: Explication]
Cultural nuance is the adaptation of a message to be understood and accepted in another culture üåç.
[image: cultural-nuance.png]

User asks ‚Äúare you available for an event next month?‚Äù
Response:
[emotion: Encouragement]
Good news üéâ! Yes, we still have availability.
Send us the event details and we will confirm quickly ‚úÖ.

User greets with ‚Äúhello‚Äù
Response:
[emotion: Salutation]
üëã Hello and welcome to Translab International!
How can I assist you today? üôÇ

Technical Note:
Use available images: {available_images}
Contextual Knowledge Base: {context}
Respond to the user‚Äôs question now based on the instructions above!

Additionally, you should always speak French by default and adapt to the visitor‚Äôs language.
"""

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{question}"),
        ])
        logger.info("Template de prompt cr√©√©")


        if not llm:
            logger.warning("LLM non disponible, la cha√Æne RAG ne peut pas √™tre cr√©√©e.")
            return None

        # La cha√Æne RAG doit fournir TOUTES les variables attendues par le prompt.
        rag_chain = RunnableMap({
            "context": lambda x: "\n\n".join([doc.page_content for doc in retriever.invoke(x["question"])]),
            "question": lambda x: x["question"],
            "history": lambda x: x.get("history", []),
            "available_images": lambda x: ", ".join(AVAILABLE_IMAGES) if AVAILABLE_IMAGES else "Aucune",
            "available_carousels": lambda x: ", ".join(image_families.keys()) if image_families else "Aucune",
            "available_emotions_list": lambda x: ", ".join(available_emotions.keys()) if available_emotions else "Aucune"
        }) | prompt | llm
        
        logger.info("Cha√Æne RAG cr√©√©e avec succ√®s")
        return rag_chain
        
    except Exception as e:
        logger.error(f"Erreur lors de la cr√©ation de la cha√Æne RAG: {str(e)}")
        # Afficher le traceback pour un meilleur d√©bogage en d√©veloppement
        logger.error(traceback.format_exc())
        return None

# √âmotions disponibles pour le chatbot
AVAILABLE_EMOTIONS = {
    "Salutation": "üëã Accueil chaleureux",
    "Information": "‚ÑπÔ∏è Partage d'informations",
    "Support": "ü§ù Aide et assistance",
    "Encouragement": "üéâ Motivation positive",
    "Explication": "üí° Clarification d√©taill√©e",
    "Pr√©sentation": "‚ú® Mise en avant",
    "Inspiration": "üåü Vision motivante"
}

def get_rag_chain():
    """Fonction wrapper pour cr√©er la cha√Æne RAG avec les √©motions."""
    return create_rag_chain(available_emotions=AVAILABLE_EMOTIONS)

if __name__ == "__main__":
    print("Testing lead_graph.py locally...")
    from langchain_core.messages import AIMessage, HumanMessage
    if not os.getenv("GROQ_API_KEY"): print("Warning: GROQ_API_KEY not set.")

    print("\n--- RAG Chain Test ---")
    # Pass an empty dict for image_families for this test
    test_rag_chain = create_rag_chain({})
    if test_rag_chain:
        print("RAG chain obtained via create_rag_chain().")
        try:
            # Test 1: First question
            print("\n--- Test 1: First Question ---")
            response1 = test_rag_chain.invoke({
                "question": "Quels sont vos services ?",
                "history": []
            })
            print(f"Test RAG Response 1: '{response1.content}'")

            # Test 2: Follow-up question
            print("\n--- Test 2: Follow-up Question ---")
            response2 = test_rag_chain.invoke({
                "question": "Et pour la traduction ?",
                "history": [
                    HumanMessage(content="Quels sont vos services ?"),
                    AIMessage(content=response1.content)
                ]
            })
            print(f"Test RAG Response 2: '{response2.content}'")

        except Exception as e:
            print(f"Error invoking test RAG chain: '{e}'")
            # Print traceback for more details
            import traceback
            traceback.print_exc()
    else:
        print("RAG chain is None after get_rag_chain(). Skipping RAG test.")

    print("\n--- Lead Extraction Test ---")
    text = "Bonjour, je suis Jean Dupont. Mon email est jean.dupont@example.com et mon tel est 0123456789."
    try:
        if structured_llm:
            collected_lead = collect_lead_from_text(text)
            print(f"Extracted Lead: '{collected_lead}'")
        else:
            print("structured_llm is None, skipping lead extraction test.")
    except Exception as e:
         print(f"Error collecting lead: '{e}'")






































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































